{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arXiv Keyword Extraction\n",
    "- Analysis Pipeline with KeyBERT and Taipy\n",
    "\n",
    "https://towardsdatascience.com/arxiv-keyword-extraction-and-analysis-pipeline-with-keybert-and-taipy-2972e81d9fa4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\narXiv는 수학, 물리학, 천문학, 전산 과학, 계량 생물학, 통계학 분야의 출판 전 논문을 수집하는 웹사이트이다. \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "arXiv는 수학, 물리학, 천문학, 전산 과학, 계량 생물학, 통계학 분야의 출판 전 논문을 수집하는 웹사이트이다. \n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Keyword extraction involves automatically identifying and extracting the most relevant words from a given text\n",
    "- keyword analysis involves analyzing the keywords to gain insights into the underlying patterns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools Overview\n",
    "- arXiv API Python wrapper\n",
    "- KeyBERT\n",
    "- Taipy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arvix 1.4.3\n",
    "keybert 0.7.0\n",
    "pandas 1.5.3\n",
    "taipy 2.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 — Setup Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yml') as f:\n",
    "    cfg = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'QUERY': 'artificial intelligence',\n",
       " 'MAX_ABSTRACTS': 30,\n",
       " 'NGRAM_MIN': 1,\n",
       " 'NGRAM_MAX': 1,\n",
       " 'TOP_N': 3,\n",
       " 'DIVERSITY_ALGO': 'mmr',\n",
       " 'DIVERSITY': 0.2,\n",
       " 'NR_CANDIDATES': 20}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 — Build Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3.1) Retrieve and Save arXiv Abstracts and Metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function.py\n",
    "import arxiv\n",
    "import pandas as pd\n",
    "\n",
    "# Function 1 - Retrieve abstracts from arXiv database\n",
    "def extract_arxiv(query: str):\n",
    "    search = arxiv.Search(\n",
    "                query=query,\n",
    "                max_results=cfg['MAX_ABSTRACTS'], # No. of abstracts to retrieve (from config)\n",
    "                sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "                sort_order=arxiv.SortOrder.Descending  # Sort by latest date\n",
    "                )\n",
    "     \n",
    "    # Returns arXiv object\n",
    "    return search\n",
    "\n",
    "# Function 2 - Save abstract text and metadata in pd.DataFrame\n",
    "def save_in_dataframe(search):\n",
    "    df = pd.DataFrame([{'uid': result.entry_id.split('.')[-1],\n",
    "                        'title': result.title,\n",
    "                        'date_published': result.published,\n",
    "                        'abstract': result.summary} for result in search.results()])\n",
    "    \n",
    "# Function 3 - Preprocess data\n",
    "def preprocess_data(df: pd.DataFrame):\n",
    "    df['date_published'] = pd.to_datetime(df['date_published'])\n",
    "\n",
    "    # Create empty column to store keyword and similarity scores\n",
    "    df['keywords_and_scores'] = ''\n",
    "\n",
    "    # Create empty column to store keyword texts only\n",
    "    df['keywords'] = ''\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dask_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
